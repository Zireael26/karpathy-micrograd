# Micrograd - Neural Network from Scratch

A minimal implementation of a neural network library inspired by [Andrej Karpathy's micrograd](https://github.com/karpathy/micrograd). This project implements automatic differentiation (backpropagation) and neural networks from scratch in Python.

## ğŸ¯ Project Overview

Micrograd is a tiny Autograd engine that implements backpropagation over a dynamically built DAG (Directed Acyclic Graph). It's designed to be educational and help understand the internals of neural networks.

### Key Features
- âœ… Scalar-valued autograd engine
- âœ… Support for basic mathematical operations (+, -, *, /, **, exp, log, tanh, relu)
- âœ… Backpropagation through computational graphs
- âœ… Neural network layers (MLP - Multi-Layer Perceptron)
- âœ… Visualization of computational graphs
- âœ… Gradient checking utilities

## ğŸš€ Getting Started

### Prerequisites
- Python 3.7+
- Virtual environment (recommended)

### Installation

1. Clone the repository:
```bash
git clone <your-repo-url>
cd micrograd
```

2. Create and activate a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Usage

#### Basic Example

```python
from micrograd.engine import Value

# Create scalar values
a = Value(2.0)
b = Value(3.0)

# Perform operations
c = a * b + b**2
c.backward()  # Compute gradients

print(f"c = {c.data}")
print(f"dc/da = {a.grad}")
print(f"dc/db = {b.grad}")
```

#### Neural Network Example

```python
from micrograd.nn import MLP

# Create a 3-input, 2-hidden layer (4,4), 1-output MLP
model = MLP(3, [4, 4, 1])

# Sample input
x = [2.0, 3.0, -1.0]
y = model(x)

print(f"Output: {y}")
```

## ğŸ“ Project Structure

```
micrograd/
â”œâ”€â”€ micrograd/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py          # Core autograd engine (Value class)
â”‚   â”œâ”€â”€ nn.py             # Neural network layers
â”‚   â””â”€â”€ utils.py          # Utility functions
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_engine.py
â”‚   â”œâ”€â”€ test_nn.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_operations.py
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ micrograd.ipynb   # Main development notebook
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_engine.py

# Run with verbose output
pytest -v
```

## ğŸ“Š Examples

Check out the `examples/` directory for:
- Basic mathematical operations
- Neural network training examples
- Computational graph visualization

## ğŸ“ Learning Resources

This implementation follows along with:
- [Andrej Karpathy's micrograd tutorial](https://www.youtube.com/watch?v=VMj-3S1tku0)
- [The spelled-out intro to neural networks and backpropagation](https://www.youtube.com/watch?v=VMj-3S1tku0)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Andrej Karpathy](https://github.com/karpathy) for the original micrograd implementation
- The deep learning community for making education accessible

---

**Note**: This is an educational project designed to understand the fundamentals of neural networks and automatic differentiation. For production use, consider established libraries like PyTorch or TensorFlow.
